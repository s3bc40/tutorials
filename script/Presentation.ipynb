{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "- Marchine learning : learning strategies, representation of knowledge and application domaine.\n",
    "- Classification tasks : diagnosis of medical condition, chess prediction, weather prediction, image classification ...\n",
    "- Decision trees : one of the most popular classification techniques in data mining.\n",
    "- Outperformed by SVM (Support Vector Machine) and ensemble classifier Random Forest. But decision tree is the best to discover knowledge.\n",
    "- Advantage : easy to read and to understand.\n",
    "- Generate simple knowledge to solve difficult ones.\n",
    "\n",
    "## 2. Principal\n",
    "- Decision trees are expressed in terms of attributes, instances and classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length  sepal width  petal length  petal width          species\n",
      "0           5.1          3.5           1.4          0.2      Iris-setosa\n",
      "1           4.9          3.0           1.4          0.2      Iris-setosa\n",
      "2           4.7          3.2           1.3          0.2      Iris-setosa\n",
      "3           7.6          3.0           6.6          2.1   Iris-virginica\n",
      "4           4.9          2.5           4.5          1.7   Iris-virginica\n",
      "5           7.3          2.9           6.3          1.8   Iris-virginica\n",
      "6           6.9          3.1           4.9          1.5  Iris-versicolor\n",
      "7           5.5          2.3           4.0          1.3  Iris-versicolor\n",
      "8           6.5          2.8           4.6          1.5  Iris-versicolor\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "training_data = [\n",
    "    [\"sunny\",\"high\",\"false\",\"N\"],\n",
    "    [\"sunny\",\"high\",\"true\",\"P\"],\n",
    "    [\"sunny\",0.8,\"true\",\"P\"],\n",
    "]\n",
    "header = [\"sepal length\",\"sepal width\",\"petal length\",\"petal width\",\"species\"]\n",
    "df = pd.DataFrame(training_data, columns=header)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training dataset\n",
    "- Source : Creator: R.A. Fisher ; Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov); Date: July, 1988 ; Updated Sept 21 by C.Blake.\n",
    "- Best known database for pattern recongnition.\n",
    "- Each rows is an instance or example and each columns is an attribute or features that describe the data. The last columns refers to the predicted class or labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Family Algorithm\n",
    "- ID3 : from a decision tree iteratively till all objects are classified. Chess challenged prediction from Quinlan.\n",
    "- C4.5, C5.0 : derived from ID3 algortithm and enhancement.\n",
    "- CART : Classification and Regression Trees. Used for the presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Construction \n",
    "#### CART decision tree\n",
    "\n",
    "- First : root node reiceive all rows input from the training dataset.\n",
    "- Each node then will ask a True/False question about one of the features.\n",
    "- Response will split or partition the data in two subsets.\n",
    "- Then these subsets will become the input of two child nodes etc...\n",
    "- Goal of the questions : to unimixed the labels as we proceed down = to get the purest distrbution of labels at each nodes.\n",
    "- Problem : wich question to ask and when ? => Need to quantify how much a question unmix the amount of uncertainity at the node : **Gini impurity**.\n",
    "- Quantify how much a question reduce this uncertainity : **Information Gain**.\n",
    "- We can select the best question ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gini impurity\n",
    "- Wich question to ask ? We have to look features available at the node : candidates for portential question.\n",
    "- Impurity represents the chance to be incorrect if we randomly assign a label to an example.\n",
    "- Gini impurity = 1 - P(label)\n",
    "- Gini impurity : metrics between 0 and 1. \n",
    "    + close to 0 stand for purity\n",
    "    + close to 1 stand for impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information Gain\n",
    "- Find the question that unmixed the most the labels.\n",
    "- First : calculate the impurity of the first node.\n",
    "- Then : test each questions possible for each features value and calculate the impurity of each childs node.\n",
    "- Calculate the average impurity of childs node.\n",
    "- Subtract the parent node impurity and the avg impurity : Infrmation gain.\n",
    "- Goal is to keep track of the best question with the best information gain ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
